{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "from urllib.parse import quote\n",
    "\n",
    "doc2start = {\"bpt6k63243601\": 123, \"bpt6k62931221\": 151, \"bpt6k6286466w\": 189, \"bpt6k6393838j\": 219, \"bpt6k6331310g\": 216, \"bpt6k6292987t\": 353, \"bpt6k62906378\": 288, \"bpt6k6391515w\": 319, \"bpt6k6315927h\": 349, \"bpt6k6319106t\": 324, \"bpt6k6315985z\": 82, \"bpt6k63959929\": 82, \"bpt6k63197984\": 56, \"bpt6k6389871r\": 77, \"bpt6k6319811j\": 79, \"bpt6k6282019m\": 72, \"bpt6k6314752k\": 190, \"bpt6k6305463c\": 113, \"bpt6k6318531z\": 108, \"bpt6k6324389h\": 72, \"bpt6k63243920\": 80, \"bpt6k6309075f\": 96, \"bpt6k6333200c\": 132, \"bpt6k63243905\": 134, \"bpt6k6333170p\": 137, \"bpt6k96727875\": 135, \"bpt6k9764746t\": 99, \"bpt6k97645375\": 123, \"bpt6k9672117f\": 125, \"bpt6k9763554c\": 123, \"bpt6k9763553z\": 105, \"bpt6k9677392n\": 110, \"bpt6k9692809v\": 113, \"bpt6k9762929c\": 129, \"bpt6k9672776c\": 119, \"bpt6k9764647w\": 121, \"bpt6k9669143t\": 145, \"bpt6k9677737t\": 139, \"bpt6k9668037f\": 167, \"bpt6k96839542\": 171, \"bpt6k96762564\": 185, \"bpt6k9685861g\": 189, \"bpt6k9763471j\": 153, \"bpt6k9762899p\": 157, \"bpt6k97630871\": 11, \"bpt6k9684454n\": 235, \"bpt6k9732740w\": 239, \"bpt6k9684013b\": 189, \"bpt6k9692626p\": 305, \"bpt6k9685098r\": 281, \"bpt6k9764402m\": 329, \"bpt6k97631451\": 322, \"bpt6k9776121t\": 49, \"bpt6k9775724t\": 33, \"bpt6k97774838\": 327, \"bpt6k9780089g\": 339}\n",
    "\n",
    "def entry2url(row):\n",
    "    \"\"\"\n",
    "    Takes a row of an Annuaire csv and\n",
    "    transforms it to the corresponding Gallica url\n",
    "    \"\"\"\n",
    "    url = \"https://gallica.bnf.fr/ark:/12148/\"\n",
    "    \n",
    "    directory = row['directory']\n",
    "    page = row['page'] - doc2start[directory]\n",
    "    url += f\"{row['directory']}/f{row['page']-doc2start[row['directory']]}\"\n",
    "    \n",
    "    r_strings = []\n",
    "    if 'name' in row and pd.notna(row['name']):\n",
    "        r_strings.append(quote(row['name'].replace('.', ' ')))\n",
    "    if 'job' in row and pd.notna(row['job']):\n",
    "        r_strings.append(quote(row['job'].replace('.', ' ')))\n",
    "    if 'street' in row and pd.notna(row['street']):\n",
    "        r_strings.append(quote(row['street'].replace('.', ' ')))\n",
    "    if 'number' in row and pd.notna(row['number']):\n",
    "        r_strings.append(quote(row['number'].replace('.', ' ')))\n",
    "    \n",
    "    if len(r_strings) > 0:\n",
    "        url += f\".item.r={'%20'.join(r_strings)}.zoom\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "def add_clickable_url(bottin_dataframe):\n",
    "    bottin_dataframe = bottin_dataframe.copy()\n",
    "    bottin_dataframe['url'] = bottin_dataframe.apply(entry2url, axis=1)\n",
    "    #def make_clickable(val):\n",
    "    #    return '<a href=\"{}\">gallica url</a>'.format(val,val)\n",
    "\n",
    "    #return bottin_dataframe.style.format(make_clickable, subset=['url'])\n",
    "    return bottin_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bottin = pd.read_csv('bottin_data_groupe_6.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nettoyer le numéro de rue: garder seulement le premier nombre et éventuellement 'bis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bottin['number_clean'] = data_bottin['number'].str.extract('(^\\d+(?: ?bis)?).*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicated entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bottin.drop_duplicates(subset =['name', 'year', 'job', 'street_clean', 'number_clean'], keep = 'first', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajout des hyperliens en texte brut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bottin = add_clickable_url(data_bottin.fillna(''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un nouveau dataframe contenant name, job, street_clean, number_clean et l'entrée Wikipast en texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['page_text'] = \"* \"+\"[[\"+data_bottin['year'].astype(str)+\"]]\"\\\n",
    "+\" / [[Paris]]. \"\\\n",
    "+\"[[\"+data_bottin['name'].astype(str)+\"]]\"\\\n",
    "+\", [[\"+ data_bottin['job'].astype(str)+\"]]\"\\\n",
    "+\", exerce son activité au\"\\\n",
    "+\" \"+data_bottin['number_clean'].astype(str)+\" \"\\\n",
    "+\"[[\"+data_bottin['street_clean'].astype(str)+\"]].\"\\\n",
    "+\" [\"+data_bottin['url'].astype(str)+\"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name'] = data_bottin['name']\n",
    "df['job'] = data_bottin['job']\n",
    "df['street_clean'] = data_bottin['street_clean']\n",
    "df['number_clean'] = data_bottin['number_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()[['page_text','name','job','street_clean','number_clean']]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groupement des entrées ayant les mêmes nom, job, street_clean et number_clean en mettant les textes générés en une liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grpby = df.groupby(['name','job','street_clean','number_clean'])['page_text'].apply(list)\n",
    "df1 = df_grpby.to_frame().reset_index()\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrer les lignes ayant une seule entrée Wikipast et dont le name ne commence pas par une lettre de l'alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[df1['page_text'].apply(lambda x: len(x)>1)].copy()\n",
    "df1 = df1[df1['name'].apply(lambda x: x[0].isalpha())].copy()\n",
    "df1 = df1.reset_index()[['name','job','street_clean','number_clean','page_text']]\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concaténation des entrées Wikipast pour former un texte qui sera écrit sur les pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['page_text'] = df1['page_text'].apply(lambda l: \"\\n\".join(sorted(l)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Génération de noms pour les homonymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_name = df1['name'].astype(str)\\\n",
    "+\" (\"+ df1['job'].astype(str)+\",\"\\\n",
    "+\" \"+df1['number_clean'].astype(str)\\\n",
    "+\" \"+df1['street_clean'].astype(str)+\")\"\n",
    "\n",
    "df1['name_page'] = df1['name']\n",
    "df1.loc[df1.duplicated(['name'], False), 'name_page'] = clean_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création de dataframe avec name: le nom partagée, et name_page: les noms des pages des homonymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicateRowsDF = df1[df1.duplicated(['name'], False)].copy()\n",
    "#df2 = df1.duplicated(['name'], False), ['name']).copy()\n",
    "duplicateRowsDF['name_page'] = \"* [[\" + df1['name_page'].astype(str)+ \"]]\"\n",
    "df_grpby2 = duplicateRowsDF.groupby(['name'])['name_page'].apply(list)\n",
    "df2 = df_grpby2.to_frame().reset_index()\n",
    "df2['name_page'] = df2['name_page'].apply(lambda l: \"\\n\".join(sorted(l)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ecriture sur Wikipast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywikiapi import Site, ApiError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour écrire les pages des personnes sur Wikipast. Retourne un dictionnaire avec les pages écrites, éditées et dont l'écriture/édition a échoué"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_page_to_wikipast(dataframe):\n",
    "    status = {'fail': list(), 'create': list(), 'edit': list()}\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        #liste des noms de pages, doit provenir du DataFrame df1\n",
    "        titre = row['name_page']\n",
    "\n",
    "        entries = row['page_text']\n",
    "        \n",
    "        #verifier qu'on écrase pas une page\n",
    "        try:  #La page n'existe pas on en crée une\n",
    "            site('edit', title=titre,\n",
    "                 section='new',\n",
    "                 sectiontitle='Biographie',\n",
    "                 text=entries,\n",
    "                 bot='true',\n",
    "                 token=site.token(),\n",
    "                 createonly=True)\n",
    "            status['create'].append(titre)\n",
    "\n",
    "        except ApiError: \n",
    "            #La page existait déjà, on rajoute à a fin\n",
    "            if row['name_page'] == row['name']:\n",
    "                titre = row['name_page']\\\n",
    "                +\" (\"+ row['job']+\",\"\\\n",
    "                +\" \"+row['number_clean']\\\n",
    "                +\" \"+row['street_clean']+\")\"\n",
    "                \n",
    "            try:\n",
    "                site('edit', title=titre,\n",
    "                     section='new',\n",
    "                     sectiontitle='Biographie',\n",
    "                     text=entries,\n",
    "                     bot='true',\n",
    "                     token=site.token(),\n",
    "                     createonly=True)\n",
    "                status['create'].append(titre)\n",
    "            except ApiError:\n",
    "                try:\n",
    "                    site('edit', title=titre + add_homonymes,\n",
    "                        appendtext='\\n'+entries,\n",
    "                        bot='true',\n",
    "                        token=site.token())\n",
    "                    status['edit'].append(titre)\n",
    "                except:\n",
    "                    status['fail'].append(titre)\n",
    "                    print(\"Une autre erreur est survenue (sûrement une erreur réseau)\")\n",
    "            except:\n",
    "                print(\"Une autre erreur est survenue (sûrement une erreur réseau)\")\n",
    "                status['fail'].append(titre)\n",
    "        except:\n",
    "            print(\"Une autre erreur est survenue (sûrement une erreur réseau)\")\n",
    "            status['fail'].append(titre)\n",
    "    \n",
    "    return status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour écrire les pages des homonymes sur Wikipast. Retourne un dictionnaire avec les pages écrites, éditées et dont l'écriture/édition a échoué"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_homonym_page_to_wikipast(dataframe):\n",
    "    status = {'fail': list(), 'create': list(), 'create_with_homonyme': list(), 'edit': list()}\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        #liste des noms de pages, doit provenir du DataFrame df1\n",
    "        titre = row['name']\n",
    "\n",
    "        entries = row['name_page']\n",
    "        \n",
    "        #verifier qu'on écrase pas une page\n",
    "        try:  #La page n'existe pas on en crée une\n",
    "            site('edit', title=titre,\n",
    "                 section='new',\n",
    "                 prependtext=\"Cette [[page d'homonymie]] recense différentes personnes partageant le même nom.\\n\",\n",
    "                 sectiontitle=\"Liste des homonymes\",\n",
    "                 text=entries,\n",
    "                 bot='true',\n",
    "                 token=site.token(),\n",
    "                 createonly=True)\n",
    "            status['create'].append(titre)\n",
    "\n",
    "\n",
    "        #La page existe déjà, on rajoute (Homonymes) dans le titre\n",
    "        except ApiError: \n",
    "            try:  #La page n'existe pas on en crée une\n",
    "                site('edit', title=titre+' (Homonymes)',\n",
    "                     section='new',\n",
    "                     prependtext=\"Cette [[page d'homonymie]] recense différentes personnes partageant le même nom.\\n\",\n",
    "                     sectiontitle=\"Liste des homonymes\",\n",
    "                     text=entries,\n",
    "                     bot='true',\n",
    "                     token=site.token(),\n",
    "                     createonly=True)\n",
    "                status['create_with_homonyme'].append(titre)\n",
    "            #La page existait déjà, on rajoute à la fin    \n",
    "            except ApiError:\n",
    "                try:\n",
    "                    site('edit', title=titre+' (Homonymes)',\n",
    "                        appendtext='\\n'+entries,\n",
    "                        bot='true',\n",
    "                        token=site.token())\n",
    "                    status['edit'].append(titre)\n",
    "                except:\n",
    "                    status['fail'].append(titre)\n",
    "                    print(\"Une autre erreur est survenue (sûrement une erreur réseau)\")\n",
    "        except:\n",
    "            status['fail'].append(titre)\n",
    "            print(\"Une autre erreur est survenue (sûrement une erreur réseau)\")\n",
    "            #Une autre erreur est survenue (sûrement une erreur réseau)\n",
    "            #devrait garder l'index d'entrées qui n'a pas réussi, pour ré-éssayer plus tard\n",
    "\n",
    "    return status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création du lien avec Wikipast et écriture des 32 premières entrées générées + la première entrée pour les pages homonymes (les 32 premières entrées couvrent en intégralité la première occurence dans la liste de noms d'homonymes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'Clement.lhoste@BottinBot6'\n",
    "password = 'q0fimgpdbef61a9lbkiumrhtpq2prmi8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = Site('http://wikipast.epfl.ch/wikipast/api.php') # Définition de l'adresse de l'API\n",
    "site.no_ssl = True # Désactivation du https, car pas activé sur wikipast\n",
    "site.login(user, password) # Login du bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "to_import = df1[41:10000]\n",
    "to_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_writing_res = write_page_to_wikipast(to_import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "page_writing_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#homonym_writing_res = write_homonym_page_to_wikipast(df2.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homonym_writing_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
