{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bot_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "from urllib.parse import quote\n",
    "\n",
    "doc2start = {\"bpt6k63243601\": 123, \"bpt6k62931221\": 151, \"bpt6k6286466w\": 189, \"bpt6k6393838j\": 219, \"bpt6k6331310g\": 216, \"bpt6k6292987t\": 353, \"bpt6k62906378\": 288, \"bpt6k6391515w\": 319, \"bpt6k6315927h\": 349, \"bpt6k6319106t\": 324, \"bpt6k6315985z\": 82, \"bpt6k63959929\": 82, \"bpt6k63197984\": 56, \"bpt6k6389871r\": 77, \"bpt6k6319811j\": 79, \"bpt6k6282019m\": 72, \"bpt6k6314752k\": 190, \"bpt6k6305463c\": 113, \"bpt6k6318531z\": 108, \"bpt6k6324389h\": 72, \"bpt6k63243920\": 80, \"bpt6k6309075f\": 96, \"bpt6k6333200c\": 132, \"bpt6k63243905\": 134, \"bpt6k6333170p\": 137, \"bpt6k96727875\": 135, \"bpt6k9764746t\": 99, \"bpt6k97645375\": 123, \"bpt6k9672117f\": 125, \"bpt6k9763554c\": 123, \"bpt6k9763553z\": 105, \"bpt6k9677392n\": 110, \"bpt6k9692809v\": 113, \"bpt6k9762929c\": 129, \"bpt6k9672776c\": 119, \"bpt6k9764647w\": 121, \"bpt6k9669143t\": 145, \"bpt6k9677737t\": 139, \"bpt6k9668037f\": 167, \"bpt6k96839542\": 171, \"bpt6k96762564\": 185, \"bpt6k9685861g\": 189, \"bpt6k9763471j\": 153, \"bpt6k9762899p\": 157, \"bpt6k97630871\": 11, \"bpt6k9684454n\": 235, \"bpt6k9732740w\": 239, \"bpt6k9684013b\": 189, \"bpt6k9692626p\": 305, \"bpt6k9685098r\": 281, \"bpt6k9764402m\": 329, \"bpt6k97631451\": 322, \"bpt6k9776121t\": 49, \"bpt6k9775724t\": 33, \"bpt6k97774838\": 327, \"bpt6k9780089g\": 339}\n",
    "\n",
    "def entry2url(row):\n",
    "    \"\"\"\n",
    "    Takes a row of an Annuaire csv and\n",
    "    transforms it to the corresponding Gallica url\n",
    "    \"\"\"\n",
    "    url = \"https://gallica.bnf.fr/ark:/12148/\"\n",
    "    \n",
    "    directory = row['directory']\n",
    "    page = row['page'] - doc2start[directory]\n",
    "    url += f\"{row['directory']}/f{row['page']-doc2start[row['directory']]}\"\n",
    "    \n",
    "    r_strings = []\n",
    "    if 'name' in row and pd.notna(row['name']):\n",
    "        r_strings.append(quote(row['name'].replace('.', ' ')))\n",
    "    if 'job' in row and pd.notna(row['job']):\n",
    "        r_strings.append(quote(row['job'].replace('.', ' ')))\n",
    "    if 'street' in row and pd.notna(row['street']):\n",
    "        r_strings.append(quote(row['street'].replace('.', ' ')))\n",
    "    if 'number' in row and pd.notna(row['number']):\n",
    "        r_strings.append(quote(row['number'].replace('.', ' ')))\n",
    "    \n",
    "    if len(r_strings) > 0:\n",
    "        url += f\".item.r={'%20'.join(r_strings)}.zoom\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "def add_clickable_url(bottin_dataframe):\n",
    "    bottin_dataframe = bottin_dataframe.copy()\n",
    "    bottin_dataframe['url'] = bottin_dataframe.apply(entry2url, axis=1)\n",
    "    #def make_clickable(val):\n",
    "    #    return '<a href=\"{}\">gallica url</a>'.format(val,val)\n",
    "\n",
    "    #return bottin_dataframe.style.format(make_clickable, subset=['url'])\n",
    "    return bottin_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bottin = pd.read_csv('bottin_data_groupe_6.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aperçu des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infos de base\n",
    "\n",
    "entries_number = data_bottin.shape[0] \n",
    "entries_per_year = data_bottin.groupby('year').size() \n",
    "unique_names_number = data_bottin['name'].unique().size \n",
    "unique_jobs_number = data_bottin['job'].unique().size \n",
    "unique_streets_number = data_bottin['street'].unique().size \n",
    "\n",
    "print(f\"Il y {entries_number} entrées, dont {unique_names_number} noms uniques, {unique_jobs_number} métiers uniques, {unique_streets_number} rue uniques\")\n",
    "print(\"\\nLa distribution d'entrées par année est la suivante:\")\n",
    "print(\"\\n\".join([f\"\\t{year}: {count}\" for year, count in entries_per_year.reset_index().values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogramme par années\n",
    "\n",
    "(data_bottin.groupby('year').size()\n",
    " .plot(kind='bar', title='Entries per year', figsize=(8,5)).set_ylabel('Number of entries'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distibrution de duplicats de noms\n",
    "\n",
    "(data_bottin['name'].value_counts()\n",
    " .plot(kind='hist',loglog=True, bins=1000,\n",
    "       title='Distribution of duplicate names',\n",
    "       figsize=(8,5)).set_xlabel('Number of duplicates'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Début de clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#garder seulement les entrées composées d'un mot (et contenu entre paranthèses)\n",
    "\n",
    "regex_one_word = '^\\s*\\w+(?:\\s?\\(.*\\)\\s*)?\\s*$'\n",
    "\n",
    "predicate_one_word = data_bottin['name'].str.match(regex_one_word) # à remplacer, pour le moment sélélectionne tout\n",
    "\n",
    "data_bottin_one_word = data_bottin.loc[predicate_one_word].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nettoyer le numéro de rue: garder seulement le premier nombre et éventuellement 'bis'\n",
    "data_bottin['number_clean'] = data_bottin['number'].str.extract('(^\\d+(?: ?bis)?).*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraire le compte de chaque mot entre (), dans name\n",
    "regex_parens = '^.*?\\((.*)\\).*?$'\n",
    "\n",
    "name_parens = data_bottin['name'].str.extract(regex_parens).dropna()[0]\n",
    "\n",
    "from collections import Counter\n",
    "name_parens_split = name_parens.str.split('\\W')\n",
    "name_parens_split = name_parens_split.apply(lambda words: [word for word in words if len(word) > 0])\n",
    "\n",
    "word_counts = Counter()\n",
    "\n",
    "for words in name_parens_split.values:\n",
    "    word_counts.update(words)\n",
    "\n",
    "#afficher les 5 mots les plus communs entre (), dans name    \n",
    "word_counts.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraire les mots correspondent pas à predicate_one_word (un seul mot + texte entre paranthèse)\n",
    "from collections import Counter\n",
    "name_one_word_split = data_bottin.loc[~predicate_one_word]['name'].str.split('\\W')\n",
    "name_one_word_split = name_one_word_split.apply(lambda words: [word for word in words if len(word) > 0])\n",
    "\n",
    "word_counts = Counter()\n",
    "\n",
    "for words in name_one_word_split.values:\n",
    "    word_counts.update(words)\n",
    "    \n",
    "#afficher les 5 expressions les plus communs présents dans les cases name de plus d'un mot      \n",
    "word_counts.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouper les duplicates et les afficher\n",
    "(data_bottin\n",
    " .groupby(['name', 'job', 'street', 'number'])\n",
    " .size()\n",
    " .sort_values(ascending=False)\n",
    " .value_counts()\n",
    " .to_frame('Count')\n",
    " .rename_axis('Number of duplicates'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meilleurs résultats avec number_clean\n",
    "(data_bottin\n",
    " .groupby(['name', 'street_only', 'number_clean'])\n",
    " .size()\n",
    " .sort_values(ascending=False)\n",
    " .value_counts()\n",
    " .to_frame('Count')\n",
    " .rename_axis('Number of duplicates'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests pour voir erreurs d'OCR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_bottin\n",
    " .groupby(['name', 'job', 'street_only', 'number_clean'])\n",
    " .size().to_frame('count')\n",
    " .sort_values(ascending=False, by=\"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bottin[((data_bottin['name'] == 'Pitt et Scott') | (data_bottin['name'] == 'Pitt et Seott')) & (data_bottin['number_clean'] == '7')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_bottin.loc[~predicate_one_word]\n",
    " .groupby(['name', 'job'])\n",
    " .size().to_frame('count')\n",
    " .sort_values(ascending=False, by=\"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_bottin[['job']].copy().drop_duplicates().reset_index()[['job']]\n",
    "x.head(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_bottin.copy().dropna()\n",
    "sample = y[y['job'].str.contains(\"success\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_clickable_url(sample.fillna(''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "que faire avec les entreprises ?\n",
    "les NC et les abréviations semblables ?\n",
    "certaines entrées des entrées mal faite\n",
    "\n",
    "dans job:\n",
    "successeur): \n",
    "success.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bottin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bottin.loc[data_bottin.duplicated(['name', 'year', 'job', 'street_clean', 'number_clean'], False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates entries\n",
    "data_bottin.drop_duplicates(subset =['name', 'year', 'job', 'street_clean', 'number_clean'], keep = 'first', inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bottin.loc[data_bottin.duplicated(['name', 'year', 'job', 'street_clean', 'number_clean'], False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bottin = add_clickable_url(data_bottin.fillna(''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[year] / Paris. [name], [job], exerce son activité au [number_clean] [street_clean]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate text for wikipast pages\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['criteria'] = data_bottin['name'].astype(str)+\" \"\\\n",
    "#+data_bottin['job'].astype(str)+\" \"\\\n",
    "#+data_bottin['number_clean'].astype(str)+\" \"\\\n",
    "#+data_bottin['street_clean'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['page_text'] = \"* \"+\"[[\"+data_bottin['year'].astype(str)+\"]]\"\\\n",
    "+\" / [[Paris]]. \"\\\n",
    "+\"[[\"+data_bottin['name'].astype(str)+\"]]\"\\\n",
    "+\", [[\"+ data_bottin['job'].astype(str)+\"]]\"\\\n",
    "+\", exerce son activité au\"\\\n",
    "+\" \"+data_bottin['number_clean'].astype(str)+\" \"\\\n",
    "+\"[[\"+data_bottin['street_clean'].astype(str)+\"]].\"\\\n",
    "+\" [\"+data_bottin['url'].astype(str)+\"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name'] = data_bottin['name']\n",
    "df['job'] = data_bottin['job']\n",
    "df['street_clean'] = data_bottin['street_clean']\n",
    "df['number_clean'] = data_bottin['number_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates entries\n",
    "#df.drop_duplicates(subset =['name', 'year', 'job', 'street_clean', 'number_clean'], keep = 'first', inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['name']==\"Pitt et Scott\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grpby = df.groupby(['name','job','street_clean','number_clean'])['page_text'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_grpby.to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[df1['name']==\"Pitt et Scott\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['page_text'] = df1['page_text'].apply(lambda l: \"\\n\".join(sorted(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df1.iloc[396089]['page_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[df1['name']==\"Pitt et Scott\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ne garder que les pages avec au moins 2 entrées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_name = df1['name'].astype(str)\\\n",
    "+\" (\"+ df1['job'].astype(str)+\",\"\\\n",
    "+\" \"+df1['number_clean'].astype(str)\\\n",
    "+\" \"+df1['street_clean'].astype(str)+\")\"\n",
    "df1['name_page'] = df1['name']\n",
    "df1.loc[df1.duplicated(['name'], False), 'name_page'] = clean_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicateRowsDF = df1[df1.duplicated(['name'], False)].copy()\n",
    "#df2 = df1.duplicated(['name'], False), ['name']).copy()\n",
    "duplicateRowsDF['name_page'] = \"* [[\" + df1['name_page'].astype(str)+ \"]]\"\n",
    "df_grpby2 = duplicateRowsDF.groupby(['name'])['name_page'].apply(list)\n",
    "df2 = df_grpby2.to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[df2['name']==\"Pitt et Scott\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['name_page'] = df2['name_page'].apply(lambda l: \"\\n\".join(sorted(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[df1['name']==\"Pitt et Scott\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chaine = \"*) el Cle\"\n",
    "chaine[re.search(\"\\w\", chaine).start(): len(chaine)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Créer une page Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'Clement.lhoste@BottinBot6'\n",
    "password = 'q0fimgpdbef61a9lbkiumrhtpq2prmi8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywikiapi import Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = Site('http://wikipast.epfl.ch/wikipast/api.php') # Définition de l'adresse de l'API\n",
    "site.no_ssl = True # Désactivation du https, car pas activé sur wikipast\n",
    "site.login(user, password) # Login du bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywikiapi import ApiError\n",
    "\n",
    "#liste des noms de pages, doit provenir du DataFrame df1\n",
    "titre = 'BottinBot6'\n",
    "\n",
    "entries = \"* [[{year}]]] / [[Paris]]. [[{name}]], [[{job}]], exerce son activité au {nb_clean} [[{street_clean}]] [{url}]\" \n",
    "\n",
    "#verifier qu'on écrase pas une page\n",
    "try:  #La page n'existe pas on en crée une\n",
    "    site('edit', title=titre,\n",
    "         section='new',\n",
    "         sectiontitle='Biographie',\n",
    "         text=entries,\n",
    "         bot='true',\n",
    "         token=site.token(),\n",
    "         createonly=True)\n",
    "    \n",
    "except ApiError: \n",
    "    #La page existait déjà, on rajoute à a fin\n",
    "    #on devrait verifier que bien la même personne -> adresse/années?\n",
    "    site('edit', title=titre,\n",
    "    section=1,\n",
    "    appendtext='\\n'+entries,\n",
    "    bot='true',\n",
    "    token=site.token())\n",
    "except:\n",
    "    print(\"Une autre erreur est survenue (sûrement une erreur réseau)\")\n",
    "    #Une autre erreur est survenue (sûrement une erreur réseau)\n",
    "    #devrait garder l'index d'entrées qui n'a pas réussi, pour ré-éssayer plus tard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création de pages d'homonymes\n",
    "\n",
    "#nom de la page, doit provenir du DataFrame df2 \n",
    "titre = 'BottinBot6'\n",
    "\n",
    "#liste des noms de pages des personnes qui ont le même nom\n",
    "entries = \"* [[\"\"Turlan (charbonnier et vins, 16 rue Beaurepaire)]]\\n* [[\"\"Turlan (charbonnier, 3 rue des Abbesses)]]\"\n",
    "\n",
    "#verifier qu'on écrase pas une page\n",
    "try:  #La page n'existe pas on en crée une\n",
    "    site('edit', title=titre,\n",
    "         section='new',\n",
    "         prependtext=\"Cette [[page d'homonymie]] recense différentes personnes partageant le même nom.\\n\",\n",
    "         sectiontitle=\"Liste des homonymes\",\n",
    "         text=entries,\n",
    "         bot='true',\n",
    "         token=site.token(),\n",
    "         createonly=True)\n",
    "\n",
    "#La page existe déjà, on rajoute (Homonymes) dans le titre\n",
    "except ApiError: \n",
    "    try:  #La page n'existe pas on en crée une\n",
    "        site('edit', title=titre+' (Homonymes)',\n",
    "             section='new',\n",
    "             sectiontitle=\"Liste des homonymes\",\n",
    "             text=entries,\n",
    "             bot='true',\n",
    "             token=site.token(),\n",
    "             createonly=True)\n",
    "    #La page existait déjà, on rajoute à la fin    \n",
    "    except ApiError: \n",
    "        site('edit', title=titre+' (Homonymes)',\n",
    "        #section=1,\n",
    "        appendtext='\\n'+entries,\n",
    "        bot='true',\n",
    "        token=site.token())\n",
    "except:\n",
    "    print(\"Une autre erreur est survenue (sûrement une erreur réseau)\")\n",
    "    #Une autre erreur est survenue (sûrement une erreur réseau)\n",
    "    #devrait garder l'index d'entrées qui n'a pas réussi, pour ré-éssayer plus tard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
